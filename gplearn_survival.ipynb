{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7094c1b6",
   "metadata": {},
   "source": [
    "# gplearn Survival\n",
    "\n",
    "This notebook explores how to implement proportional hazards models with symbolic regression (with `gplearn`). This is a pre-requisite to doing it with SHAREs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca63964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b63361",
   "metadata": {},
   "source": [
    "## 0. Test Data\n",
    "\n",
    "We load the example Rossi dataset from `lifelines`, on which Cox PH achieves C-index of ~0.64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa223f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survshares.datasets import Rossi, Metabric, GBSG2\n",
    "\n",
    "\n",
    "# Test dataset - Cox achieves C-index of 0.6403292470997135\n",
    "X, T, E = Rossi().load(normalise=True)\n",
    "feature_names = Rossi.features\n",
    "calibration_time = Rossi.tmax\n",
    "\n",
    "# Test dataset - Cox achieves C-index of 0.6391503770136181\n",
    "# X, T, E = Metabric().load(normalise=True)\n",
    "# feature_names = Metabric.features\n",
    "# calibration_time = Metabric.tmax\n",
    "\n",
    "# Test dataset - Cox achieves C-index of 0.5437638695233794\n",
    "# X, T, E = GBSG2().load(normalise=True)\n",
    "# feature_names = GBSG2.features\n",
    "# calibration_time = GBSG2.tmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83dbe64",
   "metadata": {},
   "source": [
    "## 1. Baseline Model - Cox PH\n",
    "\n",
    "We fit a standard Cox PH as the model to beat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af7d172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'time'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'event'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-658.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2025-05-16 09:47:33 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">cmp to</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fin</th>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wexp</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mar</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paro</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prio</th>\n",
       "      <td>0.26</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.19</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>9.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>1331.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>33.27 on 7 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>15.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrrr}\n",
       " & coef & exp(coef) & se(coef) & coef lower 95% & coef upper 95% & exp(coef) lower 95% & exp(coef) upper 95% & cmp to & z & p & -log2(p) \\\\\n",
       "covariate &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
       "fin & -0.19 & 0.83 & 0.10 & -0.38 & -0.00 & 0.69 & 1.00 & 0.00 & -1.98 & 0.05 & 4.40 \\\\\n",
       "age & -0.35 & 0.70 & 0.13 & -0.61 & -0.09 & 0.54 & 0.92 & 0.00 & -2.61 & 0.01 & 6.79 \\\\\n",
       "race & 0.10 & 1.11 & 0.10 & -0.10 & 0.30 & 0.91 & 1.35 & 0.00 & 1.02 & 0.31 & 1.70 \\\\\n",
       "wexp & -0.07 & 0.93 & 0.11 & -0.28 & 0.13 & 0.76 & 1.14 & 0.00 & -0.71 & 0.48 & 1.06 \\\\\n",
       "mar & -0.14 & 0.87 & 0.13 & -0.39 & 0.10 & 0.68 & 1.11 & 0.00 & -1.14 & 0.26 & 1.97 \\\\\n",
       "paro & -0.04 & 0.96 & 0.10 & -0.23 & 0.15 & 0.80 & 1.16 & 0.00 & -0.43 & 0.66 & 0.59 \\\\\n",
       "prio & 0.26 & 1.30 & 0.08 & 0.10 & 0.43 & 1.11 & 1.53 & 0.00 & 3.19 & 0.00 & 9.48 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 432 total observations, 318 right-censored observations>\n",
       "             duration col = 'time'\n",
       "                event col = 'event'\n",
       "      baseline estimation = breslow\n",
       "   number of observations = 432\n",
       "number of events observed = 114\n",
       "   partial log-likelihood = -658.75\n",
       "         time fit was run = 2025-05-16 09:47:33 UTC\n",
       "\n",
       "---\n",
       "           coef exp(coef)  se(coef)  coef lower 95%  coef upper 95% exp(coef) lower 95% exp(coef) upper 95%\n",
       "covariate                                                                                                  \n",
       "fin       -0.19      0.83      0.10           -0.38           -0.00                0.69                1.00\n",
       "age       -0.35      0.70      0.13           -0.61           -0.09                0.54                0.92\n",
       "race       0.10      1.11      0.10           -0.10            0.30                0.91                1.35\n",
       "wexp      -0.07      0.93      0.11           -0.28            0.13                0.76                1.14\n",
       "mar       -0.14      0.87      0.13           -0.39            0.10                0.68                1.11\n",
       "paro      -0.04      0.96      0.10           -0.23            0.15                0.80                1.16\n",
       "prio       0.26      1.30      0.08            0.10            0.43                1.11                1.53\n",
       "\n",
       "           cmp to     z      p  -log2(p)\n",
       "covariate                               \n",
       "fin          0.00 -1.98   0.05      4.40\n",
       "age          0.00 -2.61   0.01      6.79\n",
       "race         0.00  1.02   0.31      1.70\n",
       "wexp         0.00 -0.71   0.48      1.06\n",
       "mar          0.00 -1.14   0.26      1.97\n",
       "paro         0.00 -0.43   0.66      0.59\n",
       "prio         0.00  3.19 <0.005      9.48\n",
       "---\n",
       "Concordance = 0.64\n",
       "Partial AIC = 1331.50\n",
       "log-likelihood ratio test = 33.27 on 7 df\n",
       "-log2(p) of ll-ratio test = 15.37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.6403292470997135\n",
      "IBS: 0.09467578840277145\n"
     ]
    }
   ],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=feature_names).assign(time=T, event=E)\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(X_df, duration_col=\"time\", event_col=\"event\")\n",
    "cph.print_summary()\n",
    "\n",
    "# Double check the concordance\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "c_index = concordance_index(T, -cph.predict_partial_hazard(X), E)\n",
    "y, t = Surv.from_arrays(E, T), np.unique(T).astype(float)[1:-1]\n",
    "ibs = integrated_brier_score(y, y, cph.predict_survival_function(X, times=t).T, t)\n",
    "print(f\"C-index: {c_index}\")\n",
    "print(f\"IBS: {ibs}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c85f22",
   "metadata": {},
   "source": [
    "## 2. Fitness Metrics\n",
    "There are 3 options: \n",
    " - The concordance (c-index) \n",
    " - The DeepSurv/CoxTime ranking loss\n",
    " - The Cox partial likelihood (very similar)\n",
    "\n",
    "DeepSurv/CoxTime also includes a shrink penalty in its loss to penalise the neural networks $g(t|x)$ from having values too far from 0. \n",
    "\n",
    "`gplearn` fitness functions normally depend only on `y_true` and `y_pred`, but we also require the event/censoring indicator. We pass the indicator as the `sample_weight` as a workaround to avoid modifying `gplearn` yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c96167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-index\n",
    "from lifelines.utils import concordance_index\n",
    "from gplearn_clean.gplearn.fitness import make_fitness\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "\n",
    "def metric_c_index(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Protected concordance score metric for gplearn. Greater is better.\n",
    "    \"\"\"\n",
    "    # y_true is the event time, y_pred is the predicted risk\n",
    "    # sample_weight is the event indicator\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "        try:\n",
    "            return concordance_index(y_true, np.exp(y_pred), sample_weight)\n",
    "        except ZeroDivisionError:  # In case of no unambigous pairs\n",
    "            return 0.5\n",
    "        except RuntimeWarning:  # In case of invalid log or exp overflow\n",
    "            return 0.5\n",
    "\n",
    "scorer_c_index = make_scorer(metric_c_index, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55502897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_ranking_loss(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Ranking loss metric for gplearn (similar to DeepSurv and CoxTime). Less is better.\n",
    "    \"\"\"\n",
    "    # y_pred should be the LOG partial hazards - i.e. we admit negative vaues\n",
    "    # Inputs must be sorted by descending event time\n",
    "    ordering = np.argsort(y_true)[::-1]\n",
    "    y_true = y_true[ordering]\n",
    "    y_pred = pd.Series(y_pred[ordering])\n",
    "    sample_weight = pd.Series(sample_weight[ordering])\n",
    "\n",
    "    gamma = y_pred.max()\n",
    "    eps = 1e-7\n",
    "\n",
    "    try:\n",
    "        log_cumsum_h = np.log(np.exp(y_pred.sub(gamma)).cumsum(0).add(eps)).add(gamma)\n",
    "        pll = y_pred.sub(log_cumsum_h).mul(sample_weight).sum() / sample_weight.sum()\n",
    "        return -pll  # Flip the sign to make it a minimization function\n",
    "    except RuntimeWarning:  # In case of invalid log or exp overflow\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d5b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.evaluation.metrics import partial_log_likelihood_ph\n",
    "\n",
    "def metric_partial_likelihood(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Cox partial likelihood metric for gplearn. Less is better.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "        # We want to minimise the negative partial log likelihood\n",
    "        # y_pred should be the LOG partial hazards - i.e. we admit negative vaues\n",
    "        try:\n",
    "            pll = partial_log_likelihood_ph(y_pred, y_true, sample_weight, mean=True)\n",
    "            return -pll  # Flip the sign to make it a minimization function\n",
    "        except RuntimeWarning:  # In case of invalid log or exp overflow\n",
    "            return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23cdac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "def metric_integrated_brier(surv_pred, E_train, T_train, E_test=None, T_test=None):\n",
    "    \"\"\"\n",
    "    Integrated Brier score for pycox-type models\n",
    "    \"\"\"\n",
    "    E_test = E_test if E_test is not None else E_train \n",
    "    T_test = T_test if T_test is not None else T_train\n",
    "\n",
    "    y_train, y_test = Surv.from_arrays(E_train, T_train), Surv.from_arrays(E_test, T_test)\n",
    "\n",
    "    times = surv_pred.index.values[1:-1]\n",
    "    times = times[(times > T_test.min()) & (times < T_test.max())]\n",
    "    surv_pred = surv_pred.loc[times, :].T\n",
    "\n",
    "    return integrated_brier_score(y_train, y_test, surv_pred, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "114d2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "import warnings \n",
    "def metric_torchsurv_pll(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Cox partial likelihood metric for gplearn. Less is better.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        return neg_partial_log_likelihood(torch.tensor(y_pred), torch.tensor(sample_weight, dtype=torch.bool), torch.tensor(y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa6ea511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.6403292470997135\n",
      "Ranking loss: 5.133125475631877\n",
      "pycox PLL: 5.985998659506427\n",
      "torchsurv NPLL: 13.910039084216008\n",
      "IBS: 0.09467578840277145\n"
     ]
    }
   ],
   "source": [
    "# Testing with Cox PH\n",
    "h_pred = cph.predict_partial_hazard(X)\n",
    "surv_pred = cph.predict_survival_function(X)\n",
    "print(f\"C-index: {metric_c_index(T, -h_pred, E)}\")\n",
    "print(f\"Ranking loss: {metric_ranking_loss(T, h_pred, E)}\")\n",
    "print(f\"pycox PLL: {metric_partial_likelihood(T, h_pred, E)}\")\n",
    "print(f\"torchsurv NPLL: {metric_torchsurv_pll(T, h_pred, E)}\")\n",
    "print(f\"IBS: {metric_integrated_brier(surv_pred, E, T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964dc48",
   "metadata": {},
   "source": [
    "## 3. Other Estimands - $h_0$, $H$, and $S$\n",
    "\n",
    "Given our predicted $h(t|x)$, we also need to obtain the baseline $h_0$ (which also depends on our fitted $h$), cumulative hazards $H(t|x)$ and $H_0(t|x)$, and finally the survival $S(t|x)$. \n",
    "\n",
    "The ways to obtain these are standardised irrespective of if we used Cox or SymReg for $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models.cox import _CoxPHBase\n",
    "import torchtuples as tt\n",
    "\n",
    "class CustomPH(_CoxPHBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        if isinstance(X, tt.TupleTree):\n",
    "            X = X[0]\n",
    "        return np.log(cph.predict_partial_hazard(X).to_numpy())\n",
    "\n",
    "cc = CustomPH()\n",
    "\n",
    "# This must run & store results in the cc object before predict_cumulative_hazards can run\n",
    "h0_pred = cc.compute_baseline_hazards(X, (T, E))\n",
    "h0_true = cph.baseline_hazard_[\"baseline hazard\"]\n",
    "\n",
    "H0_pred = cc.compute_baseline_cumulative_hazards(X, (T, E))\n",
    "H0_true = cph.baseline_cumulative_hazard_[\"baseline cumulative hazard\"]\n",
    "\n",
    "H_pred = cc.predict_cumulative_hazards(X[5:6])[0]  # For a random row in X\n",
    "H_true = cph.predict_cumulative_hazard(X[5:6])[0]\n",
    "\n",
    "S_pred = cc.predict_surv(X[5:6])[0]  # For a random row in X\n",
    "S_true = cph.predict_survival_function(X[5:6])[0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "ax = np.ravel(ax)\n",
    "\n",
    "pd.DataFrame(dict(pycox=h0_pred, lifelines=h0_true, delta=h0_pred - h0_true)).plot(\n",
    "    ax=ax[0], title=\"$h_0(t)$\"\n",
    ")\n",
    "pd.DataFrame(dict(pycox=H0_pred, lifelines=H0_true, delta=H0_pred - H0_true)).plot(\n",
    "    ax=ax[1], title=\"$H_0(t)$\"\n",
    ")\n",
    "pd.DataFrame(dict(pycox=H_pred, lifelines=H_true, delta=H_pred - H_true)).plot(\n",
    "    ax=ax[2], title=\"$H(t|x^*)$\"\n",
    ")\n",
    "pd.DataFrame(dict(pycox=S_pred, lifelines=S_true, delta=S_pred - S_true)).plot(\n",
    "    ax=ax[3], title=\"$S(t|x^*)$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b823bf",
   "metadata": {},
   "source": [
    "### 3.1 Survival Estimands for `SymbolicRegressor`\n",
    "\n",
    "We piggy-back off `pycox` and use their code for the above estimands. We wrap `SymbolicRegressor` in a child class of `_CoxPHBase` to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn_clean.gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pycox.models.cox import _CoxPHBase\n",
    "import torchtuples as tt\n",
    "\n",
    "\n",
    "class SymRegPH(BaseEstimator, RegressorMixin, _CoxPHBase):\n",
    "    \"\"\"\n",
    "    Wrapper for gplearn's SymbolicRegressor to use with pycox supporting functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = SymbolicRegressor(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, sample_weight, *args, **kwargs):\n",
    "        self.model.fit(X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        if isinstance(X, tt.TupleTree):\n",
    "            X = X[0]\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y, sample_weight, *args, **kwargs):\n",
    "        y_pred = self.predict(X)\n",
    "        return concordance_index(y, y_pred, sample_weight)\n",
    "\n",
    "    def silent(self):\n",
    "        return self.model.set_params(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22368ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalSymRegPH:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def score(self, X, T, E, sign=1):\n",
    "        y_pred = sign * self.model.predict(X)\n",
    "        surv_pred = self.model.predict_surv_df(X)\n",
    "        print(f\"C-index: {concordance_index(T, y_pred, E)}\")\n",
    "        print(f\"Ranking loss: {metric_ranking_loss(T, y_pred, E)}\")\n",
    "        print(f\"Partial likelihood: {metric_partial_likelihood(T, y_pred, E)}\")\n",
    "        print(f\"IBS: {metric_integrated_brier(surv_pred, E, T)}\")\n",
    "\n",
    "    def plot_estimands(self, X, T, E, h0_scale_factor=1):\n",
    "        # This must run & store results in the symreg_cidx object before predict_cumulative_hazards can run\n",
    "        h0_pred = self.model.compute_baseline_hazards(X, (T, E))\n",
    "        h0_pred_scaled = h0_pred * h0_scale_factor\n",
    "        h0_true = cph.baseline_hazard_[\"baseline hazard\"]\n",
    "        h0_corr = h0_pred.corr(h0_true, method=\"pearson\")\n",
    "\n",
    "        H0_pred = self.model.compute_baseline_cumulative_hazards(\n",
    "            baseline_hazards_=h0_pred_scaled\n",
    "        )\n",
    "        H0_true = cph.baseline_cumulative_hazard_[\"baseline cumulative hazard\"]\n",
    "        H0_corr = H0_pred.corr(H0_true, method=\"pearson\")\n",
    "\n",
    "        # Select a random row in X\n",
    "        idx = np.random.randint(0, len(X) - 1)\n",
    "        X_star = X[idx : idx + 1]\n",
    "\n",
    "        H_pred = self.model.predict_cumulative_hazards(X_star, baseline_hazards_=h0_pred_scaled)[0]  # For a random row in X\n",
    "        H_true = cph.predict_cumulative_hazard(X_star)[0]\n",
    "        H_corr = H_pred.corr(H_true, method=\"pearson\")\n",
    "\n",
    "        S_pred = self.model.predict_surv(X_star, baseline_hazards_=h0_pred_scaled)[0]  # For a random row in X\n",
    "        S_true = cph.predict_survival_function(X_star)[0]\n",
    "        S_corr = pd.Series(S_pred).corr(S_true, method=\"pearson\")\n",
    "\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        ax = np.ravel(ax)\n",
    "\n",
    "        h0_dict = dict(symreg=h0_pred, cox=h0_true, delta=h0_pred_scaled - h0_true)\n",
    "        if h0_scale_factor != 1:\n",
    "            h0_dict[\"symreg_scaled\"] = h0_pred_scaled\n",
    "\n",
    "        pd.DataFrame(h0_dict).plot(ax=ax[0], title=f\"$h_0(t)$ - corr={h0_corr:.2f}\")\n",
    "        pd.DataFrame(dict(symreg=H0_pred, cox=H0_true, delta=H0_pred - H0_true)).plot(\n",
    "            ax=ax[1], title=f\"$H_0(t)$ - corr={H0_corr:.2f}\"\n",
    "        )\n",
    "        pd.DataFrame(dict(symreg=H_pred, cox=H_true, delta=H_pred - H_true)).plot(\n",
    "            ax=ax[2], title=f\"$H(t|x^*)$ - corr={H_corr:.2f}\"\n",
    "        )\n",
    "        pd.DataFrame(dict(symreg=S_pred, cox=S_true, delta=S_pred - S_true)).plot(\n",
    "            ax=ax[3], title=f\"$S(t|x^*)$ - corr={S_corr:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e92272",
   "metadata": {},
   "source": [
    "## 4. Proportional Hazards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c955db",
   "metadata": {},
   "source": [
    "### 4.1 Concordance Fitness\n",
    "We train GPLearn with the (safe) concordance as the fitness function. This is a relative ranking metric & thus is scale- and shift-invariant, so we expect to get poor absolute risk predictions (bad calibration) but good rankings. \n",
    "\n",
    "I.e., the predicted hazards will be on the wrong scale & may be shifted. \n",
    "\n",
    "This has knock-on effects on numerical instability of downstream survival estimands like $H_0$. The calibration is therefore poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_c_index = make_fitness(function=metric_c_index, greater_is_better=True)\n",
    "\n",
    "symreg_c = SymRegPH(\n",
    "    metric=fitness_c_index,\n",
    "    population_size=500,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.7,\n",
    "    parsimony_coefficient=\"auto\",\n",
    "    feature_names=feature_names,\n",
    "    verbose=True,\n",
    ").fit(X, T, sample_weight=E)\n",
    "symreg_c.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalSymRegPH(symreg_c).score(X, T, E)\n",
    "EvalSymRegPH(symreg_c).plot_estimands(X, T, E, h0_scale_factor=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't use `cross_validate` bc it won't pass the event indicators properly\n",
    "# so we write a custom CV loop\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "\n",
    "class SurvivalCV:\n",
    "    def __init__(self, model, score, sign=1):\n",
    "        self.model = model\n",
    "        self.score = score\n",
    "        self.sign = sign\n",
    "\n",
    "    @staticmethod\n",
    "    def cv_fold(model, X, T, E, score, idx_train, idx_test, sign=1):\n",
    "        fit_time_start = time.time()\n",
    "        model.fit(X[idx_train], T[idx_train], sample_weight=E[idx_train])\n",
    "        fit_time_stop = time.time()\n",
    "\n",
    "        score_train, score_test = (\n",
    "            score(T[idx_train], sign * model.predict(X[idx_train]), E[idx_train]),\n",
    "            score(T[idx_test], sign * model.predict(X[idx_test]), E[idx_test]),\n",
    "        )\n",
    "\n",
    "        return fit_time_stop - fit_time_start, score_train, score_test\n",
    "\n",
    "    def cross_validate(self, X, T, E, cv=5, n_jobs=-1):\n",
    "        kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(self.cv_fold)(\n",
    "                self.model, X, T, E, self.score, train_idx, test_idx, self.sign\n",
    "            )\n",
    "            for train_idx, test_idx in kf.split(X)\n",
    "        )\n",
    "\n",
    "        fit_times, train_scores, test_scores = zip(*results)\n",
    "        print(f\"Mean fit time: {np.mean(fit_times):.4f}s\")\n",
    "        print(f\"Train C: {np.mean(train_scores):.4f} ± {np.std(train_scores):.4f}\")\n",
    "        print(f\"Test C: {np.mean(test_scores):.4f} ± {np.std(test_scores):.4f}\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b330c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import survival_probability_calibration\n",
    "\n",
    "surv_pred = symreg_c.predict_surv_df(X)\n",
    "fig = survival_probability_calibration(surv_pred, T, E, t0 = calibration_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_c = SurvivalCV(symreg_c.silent(), metric_c_index).cross_validate(\n",
    "    X, T, E, cv=5, n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab4494",
   "metadata": {},
   "source": [
    "### 4.2 Shrink Penalty\n",
    "\n",
    "We add a shrink penalty to the fitness function to encourage estimated $h$ values near $0$. This fixes the \"scaling\" issue. However, the calibration remains rubbish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_c_shrink(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Concordance index with shrink penalty for gplearn. Greater is better.\n",
    "    \"\"\"\n",
    "    return metric_c_index(y_true, y_pred, sample_weight) - 0.05 * np.abs(y_pred).mean()\n",
    "\n",
    "\n",
    "fitness_c_shrink = make_fitness(function=fitness_c_shrink, greater_is_better=True)\n",
    "\n",
    "symreg_cs = SymRegPH(\n",
    "    metric=fitness_c_shrink,\n",
    "    population_size=500,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.7,\n",
    "    parsimony_coefficient=0.0,\n",
    "    feature_names=feature_names,\n",
    "    verbose=True,\n",
    ").fit(X, T, sample_weight=E)\n",
    "symreg_cs.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ade00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalSymRegPH(symreg_cs).score(X, T, E)\n",
    "EvalSymRegPH(symreg_cs).plot_estimands(X, T, E)  # No scaling factor required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import survival_probability_calibration\n",
    "\n",
    "surv_pred = symreg_cs.predict_surv_df(X)\n",
    "fig = survival_probability_calibration(surv_pred, T, E, t0 = calibration_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_cs = SurvivalCV(symreg_cs.silent(), metric_c_index).cross_validate(\n",
    "    X, T, E, cv=5, n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e06252",
   "metadata": {},
   "source": [
    "### 4.3 Log-Partial Likelihood Loss\n",
    "\n",
    "This yields good discrimination and calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14957e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_pll_shrink(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Partial log-likelihood with shrink penalty for gplearn. Smaller is better.\n",
    "    \"\"\"\n",
    "    pll = metric_partial_likelihood(y_true, y_pred, sample_weight)\n",
    "    return pll + 0.05 * np.abs(y_pred).mean()\n",
    "\n",
    "\n",
    "fitness_pll_shrink = make_fitness(\n",
    "    function=fitness_pll_shrink, greater_is_better=False\n",
    ")\n",
    "\n",
    "symreg_rl = SymRegPH(\n",
    "    metric=fitness_pll_shrink,\n",
    "    population_size=500,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.7,\n",
    "    parsimony_coefficient=0.00,\n",
    "    feature_names=feature_names,\n",
    "    verbose=True,\n",
    ").fit(X, T, sample_weight=E)\n",
    "symreg_rl.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalSymRegPH(symreg_rl).score(X, T, E, sign=-1)\n",
    "EvalSymRegPH(symreg_rl).plot_estimands(X, T, E)  # No scaling factor required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_rl = SurvivalCV(symreg_rl.silent(), metric_c_index, sign=-1).cross_validate(\n",
    "    X, T, E, cv=5, n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b396711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import survival_probability_calibration\n",
    "\n",
    "surv_pred = symreg_rl.predict_surv_df(X)\n",
    "fig = survival_probability_calibration(surv_pred, T, E, t0 = calibration_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac79074",
   "metadata": {},
   "source": [
    "## 5. Non-Proportional Hazards\n",
    "\n",
    "We switch from proportional hazards form $h(t|x) = h_0(t)\\exp(g(x))$ to non-proportional hazards by allowing time to interact within $g$:\n",
    "\n",
    "$$\n",
    "h(t|x) = h_0(t)\\exp(g(t|x))\n",
    "$$\n",
    "\n",
    "We retain the $h_0$ term to ensure feature-invariant components of the survival over time don't get cancelled out in the likelihood calculation.\n",
    "\n",
    "In practice, we simply allow event time to be its own feature in the input to gplearn. This is analogous to how Cox-Time concatenates the event time to the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06a7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn_clean.gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pycox.models.cox_time import CoxTime\n",
    "import torchtuples as tt\n",
    "\n",
    "\n",
    "class SymRegNPH(BaseEstimator, RegressorMixin, CoxTime):\n",
    "    \"\"\"\n",
    "    Wrapper for gplearn's SymbolicRegressor to use with pycox supporting functions. For non-proportional hazards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: SymbolicRegressor, **kwargs):\n",
    "        self.model = model\n",
    "        self.labtrans = None\n",
    "\n",
    "    def fit(self, X, T, E, *args, **kwargs): \n",
    "        Xt = np.hstack((X, T))\n",
    "        self.model.fit(Xt, sample_weight=E)\n",
    "        self.training_data = tt.tuplefy(X, (T, E))\n",
    "        return self\n",
    "\n",
    "    def predict(self, Xt_tuple, *args, **kwargs):\n",
    "        X, t = Xt_tuple\n",
    "        if isinstance(X, tt.TupleTree):\n",
    "            X = X[0]\n",
    "        Xt = np.hstack((X, t)) # Simply add the time as another column to the input just like MLPVanillaCoxTime\n",
    "        return self.model.predict(Xt)\n",
    "\n",
    "    def silent(self):\n",
    "        return self.model.set_params(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae631d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_c_shrink(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Concordance index with shrink penalty for gplearn. Greater is better.\n",
    "    \"\"\"\n",
    "    return metric_c_index(y_true, y_pred, sample_weight) - 0.05 * np.abs(y_pred).mean()\n",
    "\n",
    "\n",
    "fitness_c_shrink = make_fitness(function=fitness_c_shrink, greater_is_better=True)\n",
    "\n",
    "symreg_cs = SymRegNPH(SymbolicRegressor(\n",
    "    metric=fitness_c_shrink,\n",
    "    population_size=500,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.7,\n",
    "    parsimony_coefficient=0.0,\n",
    "    feature_names=feature_names,\n",
    "    verbose=True,\n",
    ")).fit(X, T, sample_weight=E)\n",
    "symreg_cs.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shares",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
