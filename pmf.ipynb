{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7f9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b417fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survshares.datasets import Rossi \n",
    "dataset = Rossi()\n",
    "X, T, E = dataset.load() \n",
    "feature_names = dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4581340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patching gplearn.genetic._parallel_evolve to allow custom program classes\n",
    "import gplearn_clean.gplearn.genetic \n",
    "from gplearn_clean.gplearn.utils import check_random_state\n",
    "\n",
    "def _parallel_evolve(n_programs, parents, X, y, sample_weight, seeds, params):\n",
    "    \"\"\"Private function used to build a batch of programs within a job.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Unpack parameters\n",
    "    tournament_size = params['tournament_size']\n",
    "    function_set = params['function_set']\n",
    "    arities = params['arities']\n",
    "    init_depth = params['init_depth']\n",
    "    init_method = params['init_method']\n",
    "    const_range = params['const_range']\n",
    "    metric = params['_metric']\n",
    "    transformer = params['_transformer']\n",
    "    parsimony_coefficient = params['parsimony_coefficient']\n",
    "    method_probs = params['method_probs']\n",
    "    p_point_replace = params['p_point_replace']\n",
    "    max_samples = params['max_samples']\n",
    "    feature_names = params['feature_names']\n",
    "    program_class = params['program_class'] # Relying on BaseEstimator.get_params() to pick this up \n",
    "    tgrid = params['tgrid']\n",
    "\n",
    "    max_samples = int(max_samples * n_samples)\n",
    "\n",
    "    def _tournament():\n",
    "        \"\"\"Find the fittest individual from a sub-population.\"\"\"\n",
    "        contenders = random_state.randint(0, len(parents), tournament_size)\n",
    "        fitness = [parents[p].fitness_ for p in contenders]\n",
    "        if metric.greater_is_better:\n",
    "            parent_index = contenders[np.argmax(fitness)]\n",
    "        else:\n",
    "            parent_index = contenders[np.argmin(fitness)]\n",
    "        return parents[parent_index], parent_index\n",
    "\n",
    "    # Build programs\n",
    "    programs = []\n",
    "\n",
    "    for i in range(n_programs):\n",
    "\n",
    "        random_state = check_random_state(seeds[i])\n",
    "\n",
    "        if parents is None:\n",
    "            program = None\n",
    "            genome = None\n",
    "        else:\n",
    "            method = random_state.uniform()\n",
    "            parent, parent_index = _tournament()\n",
    "\n",
    "            if method < method_probs[0]:\n",
    "                # crossover\n",
    "                donor, donor_index = _tournament()\n",
    "                program, removed, remains = parent.crossover(donor.program,\n",
    "                                                             random_state)\n",
    "                genome = {'method': 'Crossover',\n",
    "                          'parent_idx': parent_index,\n",
    "                          'parent_nodes': removed,\n",
    "                          'donor_idx': donor_index,\n",
    "                          'donor_nodes': remains}\n",
    "            elif method < method_probs[1]:\n",
    "                # subtree_mutation\n",
    "                program, removed, _ = parent.subtree_mutation(random_state)\n",
    "                genome = {'method': 'Subtree Mutation',\n",
    "                          'parent_idx': parent_index,\n",
    "                          'parent_nodes': removed}\n",
    "            elif method < method_probs[2]:\n",
    "                # hoist_mutation\n",
    "                program, removed = parent.hoist_mutation(random_state)\n",
    "                genome = {'method': 'Hoist Mutation',\n",
    "                          'parent_idx': parent_index,\n",
    "                          'parent_nodes': removed}\n",
    "            elif method < method_probs[3]:\n",
    "                # point_mutation\n",
    "                program, mutated = parent.point_mutation(random_state)\n",
    "                genome = {'method': 'Point Mutation',\n",
    "                          'parent_idx': parent_index,\n",
    "                          'parent_nodes': mutated}\n",
    "            else:\n",
    "                # reproduction\n",
    "                program = parent.reproduce()\n",
    "                genome = {'method': 'Reproduction',\n",
    "                          'parent_idx': parent_index,\n",
    "                          'parent_nodes': []}\n",
    "\n",
    "        program = program_class(function_set=function_set,\n",
    "                           arities=arities,\n",
    "                           init_depth=init_depth,\n",
    "                           init_method=init_method,\n",
    "                           n_features=n_features,\n",
    "                           metric=metric,\n",
    "                           transformer=transformer,\n",
    "                           const_range=const_range,\n",
    "                           p_point_replace=p_point_replace,\n",
    "                           parsimony_coefficient=parsimony_coefficient,\n",
    "                           feature_names=feature_names,\n",
    "                           random_state=random_state,\n",
    "                           program=program,\n",
    "                           tgrid=tgrid)\n",
    "\n",
    "        program.parents = genome\n",
    "\n",
    "        # Draw samples, using sample weights, and then fit\n",
    "        if sample_weight is None:\n",
    "            curr_sample_weight = np.ones((n_samples,))\n",
    "        else:\n",
    "            curr_sample_weight = sample_weight.copy()\n",
    "        oob_sample_weight = curr_sample_weight.copy()\n",
    "\n",
    "        indices, not_indices = program.get_all_indices(n_samples,\n",
    "                                                       max_samples,\n",
    "                                                       random_state)\n",
    "\n",
    "        curr_sample_weight[not_indices] = 0\n",
    "        oob_sample_weight[indices] = 0\n",
    "\n",
    "        program.raw_fitness_ = program.raw_fitness(X, y, curr_sample_weight)\n",
    "        if max_samples < n_samples:\n",
    "            # Calculate OOB fitness\n",
    "            program.oob_fitness_ = program.raw_fitness(X, y, oob_sample_weight)\n",
    "\n",
    "        programs.append(program)\n",
    "\n",
    "    return programs\n",
    "\n",
    "gplearn_clean.gplearn.genetic._parallel_evolve = _parallel_evolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f230c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn_clean.gplearn._program import _Program \n",
    "\n",
    "class _TimegridProgram(_Program): \n",
    "    def __init__(self,\n",
    "                tgrid,\n",
    "                function_set,\n",
    "                arities,\n",
    "                init_depth,\n",
    "                init_method,\n",
    "                n_features,\n",
    "                const_range,\n",
    "                metric,\n",
    "                p_point_replace,\n",
    "                parsimony_coefficient,\n",
    "                random_state,\n",
    "                transformer=None,\n",
    "                feature_names=None,\n",
    "                program=None):\n",
    "        super().__init__(function_set, arities, init_depth, init_method, n_features,\n",
    "                         const_range, metric, p_point_replace, parsimony_coefficient,\n",
    "                         random_state, transformer, feature_names, program)\n",
    "        self._feature_names = feature_names\n",
    "        self._n_features_X = n_features\n",
    "        self.tgrid = tgrid\n",
    "\n",
    "    @property \n",
    "    def feature_names(self): \n",
    "        if self._feature_names is not None: \n",
    "            return list(self._feature_names) + ['time']\n",
    "        else:\n",
    "            return [f'X{i}' for i in range(self._n_features_X)] + ['time']\n",
    "        \n",
    "    @feature_names.setter\n",
    "    def feature_names(self, value):\n",
    "        self._feature_names = value\n",
    "\n",
    "    @property \n",
    "    def n_features(self):\n",
    "        return self._n_features_X + 1  # +1 for the time feature\n",
    "    \n",
    "    @n_features.setter\n",
    "    def n_features(self, value):\n",
    "        self._n_features_X = value \n",
    "\n",
    "    def execute(self, X, t = None):\n",
    "        \"\"\"Every row in X is evaluated at every time point in t. Returns a prediction matrix (n_samples, n_times)\"\"\"\n",
    "\n",
    "        if t is None: # By default use the time grid\n",
    "            t = self.tgrid\n",
    "        if isinstance(t, (int, float)): # Handle single time points\n",
    "            t = np.array([t])\n",
    "        elif isinstance(t, list):\n",
    "            t = np.array(t)\n",
    "        elif not isinstance(t, np.ndarray):\n",
    "            raise ValueError(\"t must be an int, float, list or numpy array\")\n",
    "        \n",
    "        X_expanded = np.repeat(X, len(t), axis=0) # Repeat each row of X for each time point in t\n",
    "        t_expanded = np.tile(t, X.shape[0]).reshape(-1, 1) # Corresponding time points for each row of X_expanded\n",
    "        X_expanded = np.hstack((X_expanded, t_expanded)) # Combine X and t into a single matrix\n",
    "\n",
    "        return super().execute(X_expanded).reshape(X.shape[0], len(t))\n",
    "    \n",
    "\n",
    "    def raw_fitness(self, X, T, E): \n",
    "        # EXPECTING T as IDX_DURATIONS - NOT AS DURATION TIMES\n",
    "        from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "        from pycox.models.data import pair_rank_mat\n",
    "        from pycox.models.loss import nll_pmf, rank_loss_deephit_single\n",
    "        import torch\n",
    "\n",
    "        # T, E = LabTransDiscreteTime(self.tgrid).fit_transform()\n",
    "\n",
    "        # Alpha is weighting between likelihood and rank loss (so not like in paper):\n",
    "        # loss = alpha * nll + (1 - alpha) rank_loss(sigma)\n",
    "        alpha = 0.2 # Parameter that controls the linear combination between the nll and ranking loss\n",
    "        sigma = 0.1 # Parameter used by the ranking loss \n",
    "        reduction = 'mean'\n",
    "\n",
    "        # Required:\n",
    "        # 1. phi: the predicted survival function at each time point in a matrix (n_samples, n_times)\n",
    "        # 2. idx_durations: the time indices in the grid of the observed event/censoring times\n",
    "        # 3. events: float indicator of event or censoring (1 is event)\n",
    "        # 4. rank_mat: Indicator matrix R with R_ij = 1{T_i < T_j and D_i = 1}. \n",
    "        #       So it takes value 1 if we observe that i has an event before j and zero otherwise.\n",
    "\n",
    "        phi = self.execute(X, self.tgrid)\n",
    "        rank_mat = pair_rank_mat(T, E) # Inputs must be numpy arrays \n",
    "\n",
    "        phi, T, E, rank_mat = torch.Tensor(phi), torch.tensor(T), torch.Tensor(E), torch.Tensor(rank_mat)\n",
    "\n",
    "        # 3 parts: nll_pmf, rank_loss, and penalty \n",
    "        # Inputs must be tensors\n",
    "        nll = nll_pmf(phi, T, E, reduction)\n",
    "        rank_loss = rank_loss_deephit_single(phi, T, E, rank_mat, sigma, reduction)\n",
    "\n",
    "        # Penalty: for each row in phi, sum the values and penalize deviation from [0, 1]\n",
    "        # row_sums = phi.sum(dim=1)\n",
    "        # penalty = torch.abs(torch.clamp(row_sums, 0, 1) - row_sums).sum()\n",
    "\n",
    "\n",
    "        return alpha * nll + (1. - alpha) * rank_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20077e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "transformer = LabTransDiscreteTime(np.unique(T).astype(np.float64))\n",
    "T_idx, E = transformer.transform(T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a74668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0344)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gplearn_clean.gplearn.fitness import mean_square_error\n",
    "from gplearn_clean.gplearn.functions import add2, sub2, mul2, div2\n",
    "from sklearn.utils.validation import check_random_state\n",
    "\n",
    "program_params = {\n",
    "    \"function_set\": [add2, sub2, mul2, div2],\n",
    "    \"arities\": {2: [add2, sub2, mul2, div2]},\n",
    "    \"init_depth\": (2, 6),\n",
    "    \"init_method\": \"half and half\",\n",
    "    \"n_features\": 2,\n",
    "    \"const_range\": (-1.0, 1.0),\n",
    "    \"metric\": mean_square_error,\n",
    "    \"p_point_replace\": 0.05,\n",
    "    \"parsimony_coefficient\": 0.1,\n",
    "    \"random_state\": check_random_state(415),\n",
    "}\n",
    "\n",
    "prog = [mul2, 0, 2]\n",
    "gp = _TimegridProgram(tgrid = transformer.cuts, program=prog, **program_params)\n",
    "gp.execute(np.array([[1, 2], [2, 2]]))\n",
    "gp.raw_fitness(X, T_idx, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493957ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn_clean.gplearn.genetic import SymbolicRegressor\n",
    "class TimegridRegressor(SymbolicRegressor):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 tgrid=None,\n",
    "                 program_class=_TimegridProgram,\n",
    "                 population_size=1000,\n",
    "                 generations=20,\n",
    "                 tournament_size=20,\n",
    "                 stopping_criteria=0.0,\n",
    "                 const_range=(-1., 1.),\n",
    "                 init_depth=(2, 6),\n",
    "                 init_method='half and half',\n",
    "                 function_set=('add', 'sub', 'mul', 'div'),\n",
    "                 metric='mean absolute error',\n",
    "                 parsimony_coefficient=0.001,\n",
    "                 p_crossover=0.9,\n",
    "                 p_subtree_mutation=0.01,\n",
    "                 p_hoist_mutation=0.01,\n",
    "                 p_point_mutation=0.01,\n",
    "                 p_point_replace=0.05,\n",
    "                 max_samples=1.0,\n",
    "                 feature_names=None,\n",
    "                 warm_start=False,\n",
    "                 low_memory=False,\n",
    "                 n_jobs=1,\n",
    "                 verbose=0,\n",
    "                 random_state=None):\n",
    "        super(SymbolicRegressor, self).__init__(\n",
    "            population_size=population_size,\n",
    "            generations=generations,\n",
    "            tournament_size=tournament_size,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            const_range=const_range,\n",
    "            init_depth=init_depth,\n",
    "            init_method=init_method,\n",
    "            function_set=function_set,\n",
    "            metric=metric,\n",
    "            parsimony_coefficient=parsimony_coefficient,\n",
    "            p_crossover=p_crossover,\n",
    "            p_subtree_mutation=p_subtree_mutation,\n",
    "            p_hoist_mutation=p_hoist_mutation,\n",
    "            p_point_mutation=p_point_mutation,\n",
    "            p_point_replace=p_point_replace,\n",
    "            max_samples=max_samples,\n",
    "            feature_names=feature_names,\n",
    "            warm_start=warm_start,\n",
    "            low_memory=low_memory,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            random_state=random_state)\n",
    "        \n",
    "        self.program_class = program_class # Relying on BaseEstimator.get_params() to pass this to parallel_evolve\n",
    "        self.tgrid = tgrid \n",
    "        \n",
    "from gplearn_clean.gplearn.fitness import make_fitness \n",
    "loss_fitness = make_fitness(function=lambda x, y, z: 0, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1444078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stybl/mamba/envs/shares/lib/python3.11/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    27.31           813298        3         0.704235              N/A     30.02s\n",
      "   1     8.10           4.1017        5         0.630896              N/A     25.22s\n",
      "   2     6.90          1.12539        7         0.550858              N/A     23.54s\n",
      "   3     6.34          10.1817       11          0.50344              N/A     22.12s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = TimegridRegressor(metric=loss_fitness, program_class=_TimegridProgram, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, tgrid=transformer.cuts, feature_names=feature_names)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/SurvSHAREs/gplearn_clean/gplearn/genetic.py:476\u001b[39m, in \u001b[36mBaseSymbolic.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    472\u001b[39m n_jobs, n_programs, starts = _partition_estimators(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m.population_size, \u001b[38;5;28mself\u001b[39m.n_jobs)\n\u001b[32m    474\u001b[39m seeds = random_state.randint(MAX_INT, size=\u001b[38;5;28mself\u001b[39m.population_size)\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m population = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_evolve\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_programs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                              \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                              \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# Reduce, maintaining order across different n_jobs\u001b[39;00m\n\u001b[32m    488\u001b[39m population = \u001b[38;5;28mlist\u001b[39m(itertools.chain.from_iterable(population))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/shares/lib/python3.11/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/shares/lib/python3.11/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36m_parallel_evolve\u001b[39m\u001b[34m(n_programs, parents, X, y, sample_weight, seeds, params)\u001b[39m\n\u001b[32m    114\u001b[39m curr_sample_weight[not_indices] = \u001b[32m0\u001b[39m\n\u001b[32m    115\u001b[39m oob_sample_weight[indices] = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m program.raw_fitness_ = \u001b[43mprogram\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_samples < n_samples:\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Calculate OOB fitness\u001b[39;00m\n\u001b[32m    120\u001b[39m     program.oob_fitness_ = program.raw_fitness(X, y, oob_sample_weight)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36m_TimegridProgram.raw_fitness\u001b[39m\u001b[34m(self, X, T, E)\u001b[39m\n\u001b[32m     90\u001b[39m phi, T, E, rank_mat = torch.Tensor(phi), torch.tensor(T), torch.Tensor(E), torch.Tensor(rank_mat)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# 3 parts: nll_pmf, rank_loss, and penalty \u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Inputs must be tensors\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m nll = \u001b[43mnll_pmf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m rank_loss = rank_loss_deephit_single(phi, T, E, rank_mat, sigma, reduction)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Penalty: for each row in phi, sum the values and penalize deviation from [0, 1]\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# row_sums = phi.sum(dim=1)\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# penalty = torch.abs(torch.clamp(row_sums, 0, 1) - row_sums).sum()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mamba/envs/shares/lib/python3.11/site-packages/pycox/models/loss.py:85\u001b[39m, in \u001b[36mnll_pmf\u001b[39m\u001b[34m(phi, idx_durations, events, reduction, epsilon)\u001b[39m\n\u001b[32m     83\u001b[39m phi = utils.pad_col(phi)\n\u001b[32m     84\u001b[39m gamma = phi.max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m cumsum = \u001b[43mphi\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.cumsum(\u001b[32m1\u001b[39m)\n\u001b[32m     86\u001b[39m sum_ = cumsum[:, -\u001b[32m1\u001b[39m]\n\u001b[32m     87\u001b[39m part1 = phi.gather(\u001b[32m1\u001b[39m, idx_durations).view(-\u001b[32m1\u001b[39m).sub(gamma).mul(events)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = TimegridRegressor(metric=loss_fitness, program_class=_TimegridProgram, verbose=True, tgrid=transformer.cuts, feature_names=feature_names)\n",
    "model.fit(X, T_idx, E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shares",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
